[{"num_versions": 12, "url_citation": null, "title": "A two-stage pretraining algorithm for deep boltzmann machines", "url": "http://link.springer.com/chapter/10.1007/978-3-642-40728-4_14", "url_versions": "http://scholar.google.com/scholar?cluster=972406428643825884&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "KH Cho, T Raiko, A Ilin, J Karhunen - Artificial Neural Networks and  \u2026", "excerpt": "Abstract A deep Boltzmann machine (DBM) is a recently introduced Markov random field model that has multiple layers of hidden units. It has been shown empirically that it is difficult to train a DBM with approximate maximum-likelihood learning using the stochastic  ...", "url_pdf": null, "num_citations": 9, "cluster_id": "972406428643825884", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=972406428643825884&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 4, "url_citation": null, "title": "Optimization techniques to improve training speed of deep neural networks for large speech tasks", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6619439", "url_versions": "http://scholar.google.com/scholar?cluster=8342459325676772309&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "TN Sainath, B Kingsbury, H Soltau\u2026 - Audio, Speech, and  \u2026", "excerpt": "Abstract\u2014While Deep Neural Networks (DNNs) have achieved tremendous success for large vocabulary continuous speech recognition (LVCSR) tasks, training these networks is slow. Even to date, the most common approach to train DNNs is via stochastic gradient  ...", "url_pdf": null, "num_citations": 21, "cluster_id": "8342459325676772309", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=8342459325676772309&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 16, "url_citation": null, "title": "Minimizing finite sums with the stochastic average gradient", "url": "http://arxiv.org/abs/1309.2388", "url_versions": "http://scholar.google.com/scholar?cluster=9162169044082165875&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "M Schmidt, NL Roux, F Bach - arXiv preprint arXiv:1309.2388", "excerpt": "Abstract: We propose the stochastic average gradient (SAG) method for optimizing the sum of a finite number of smooth convex functions. Like stochastic gradient (SG) methods, the SAG method's iteration cost is independent of the number of terms in the sum. However,  ...", "url_pdf": null, "num_citations": 67, "cluster_id": "9162169044082165875", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=9162169044082165875&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 9, "url_citation": null, "title": "BAMBI: blind accelerated multimodal Bayesian inference", "url": "http://mnras.oxfordjournals.org/content/421/1/169.short", "url_versions": "http://scholar.google.com/scholar?cluster=13852733719753163476&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "P Graff, F Feroz, MP Hobson\u2026 - Monthly Notices of the  \u2026", "excerpt": "Abstract In this paper, we present an algorithm for rapid Bayesian analysis that combines the benefits of nested sampling and artificial neural networks (NNs). The blind accelerated multimodal Bayesian inference (BAMBI) algorithm implements the M ulti N est package for  ...", "url_pdf": null, "num_citations": 25, "cluster_id": "13852733719753163476", "year": "2012", "url_citations": "http://scholar.google.com/scholar?cites=13852733719753163476&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 13, "url_citation": null, "title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "url": "http://dl.acm.org/citation.cfm?id=2188396", "url_versions": "http://scholar.google.com/scholar?cluster=3511489826918088557&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "MU Gutmann, A Hyv\u00e4rinen - The Journal of Machine Learning Research", "excerpt": "Abstract We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model  ...", "url_pdf": null, "num_citations": 60, "cluster_id": "3511489826918088557", "year": "2012", "url_citations": "http://scholar.google.com/scholar?cites=3511489826918088557&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 16, "url_citation": null, "title": "On the importance of initialization and momentum in deep learning", "url": "http://machinelearning.wustl.edu/mlpapers/papers/icml2013_sutskever13", "url_versions": "http://scholar.google.com/scholar?cluster=7449004388220998591&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "I Sutskever, J Martens, G Dahl\u2026 - Proceedings of the  \u2026", "excerpt": "Abstract: Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent  ...", "url_pdf": null, "num_citations": 154, "cluster_id": "7449004388220998591", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=7449004388220998591&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 20, "url_citation": null, "title": "A stochastic gradient method with an exponential convergence _rate for finite training sets", "url": "http://papers.nips.cc/paper/4633-a-stochastic-gradient-method-with-an-exponential-convergence-_rate-for-finite-training-sets", "url_versions": "http://scholar.google.com/scholar?cluster=1209759207862565895&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "NL Roux, M Schmidt, FR Bach - Advances in Neural Information  \u2026", "excerpt": "Abstract We propose a new stochastic gradient method for optimizing the sum of a finite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a  ...", "url_pdf": null, "num_citations": 113, "cluster_id": "1209759207862565895", "year": "2012", "url_citations": "http://scholar.google.com/scholar?cites=1209759207862565895&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 10, "url_citation": null, "title": "Deep learning of representations: Looking forward", "url": "http://link.springer.com/chapter/10.1007/978-3-642-39593-2_1", "url_versions": "http://scholar.google.com/scholar?cluster=16988628068303769209&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "Y Bengio - Statistical Language and Speech Processing", "excerpt": "Abstract Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts. Although the study of deep learning has already led to impressive theoretical  ...", "url_pdf": null, "num_citations": 86, "cluster_id": "16988628068303769209", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=16988628068303769209&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 16, "url_citation": null, "title": "New types of deep neural network learning for speech recognition and related applications: An overview", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6639344", "url_versions": "http://scholar.google.com/scholar?cluster=10871829039953032247&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "L Deng, G Hinton, B Kingsbury - Acoustics, Speech and Signal  \u2026", "excerpt": "ABSTRACT In this paper, we provide an overview of the invited and contributed papers presented at the special session at ICASSP-2013, entitled \u201cNew Types of Deep Neural Network Learning for Speech Recognition and Related Applications,\u201d as organized by the  ...", "url_pdf": null, "num_citations": 82, "cluster_id": "10871829039953032247", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=10871829039953032247&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 3, "url_citation": null, "title": "On optimization methods for deep learning", "url": "http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Le_210.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=14410383800974641198&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "J Ngiam, A Coates, A Lahiri\u2026 - \u2026  of the 28th  \u2026", "excerpt": "Abstract The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug  ...", "url_pdf": "http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Le_210.pdf", "num_citations": 152, "cluster_id": "14410383800974641198", "year": "2011", "url_citations": "http://scholar.google.com/scholar?cites=14410383800974641198&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 14, "url_citation": null, "title": "Deep learning in neural networks: An overview", "url": "http://www.sciencedirect.com/science/article/pii/S0893608014002135", "url_versions": "http://scholar.google.com/scholar?cluster=15932869302045479284&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "J Schmidhuber - Neural Networks", "excerpt": "Abstract In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow  ...", "url_pdf": null, "num_citations": 143, "cluster_id": "15932869302045479284", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=15932869302045479284&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 4, "url_citation": null, "title": "Revisiting natural gradient for deep networks", "url": "http://arxiv.org/abs/1301.3584", "url_versions": "http://scholar.google.com/scholar?cluster=4887476036427842105&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "R Pascanu, Y Bengio - arXiv preprint arXiv:1301.3584", "excerpt": "Abstract: We evaluate natural gradient, an algorithm originally proposed in Amari (1997), for learning deep models. The contributions of this paper are as follows. We show the connection between natural gradient and three other recently proposed methods for  ...", "url_pdf": null, "num_citations": 30, "cluster_id": "4887476036427842105", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=4887476036427842105&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 6, "url_citation": null, "title": "Neural circuits as computational dynamical systems", "url": "http://www.sciencedirect.com/science/article/pii/S0959438814000166", "url_versions": "http://scholar.google.com/scholar?cluster=5784002942556450815&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "D Sussillo - Current opinion in neurobiology", "excerpt": "Introduction Systems neuroscience is heading towards the simultaneous recording or imaging of many neurons, often while an animal engages in a complicated behavior [1, 2, 3, 4, 5, 63]. This trend has led to a wealth of data that requires new conceptual approaches,  ...", "url_pdf": null, "num_citations": 12, "cluster_id": "5784002942556450815", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=5784002942556450815&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 10, "url_citation": null, "title": "Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods", "url": "http://arxiv.org/abs/1311.2115", "url_versions": "http://scholar.google.com/scholar?cluster=5987479414880920587&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "J Sohl-Dickstein, B Poole, S Ganguli - arXiv preprint arXiv:1311.2115", "excerpt": "Abstract: We present an algorithm for minimizing a sum of functions that combines the computational efficiency of stochastic gradient descent (SGD) with the second order curvature information leveraged by quasi-Newton methods. We unify these disparate  ...", "url_pdf": null, "num_citations": 12, "cluster_id": "5987479414880920587", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=5987479414880920587&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 6, "url_citation": null, "title": "On the use of stochastic hessian information in optimization methods for machine learning", "url": "http://epubs.siam.org/doi/abs/10.1137/10079923X", "url_versions": "http://scholar.google.com/scholar?cluster=11833932512235803003&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "RH Byrd, GM Chin, W Neveitt, J Nocedal - SIAM Journal on Optimization", "excerpt": "This paper describes how to incorporate sampled curvature information in a Newton-CG method and in a limited memory quasi-Newton method for statistical learning. The motivation for this work stems from supervised machine learning applications involving a  ...", "url_pdf": null, "num_citations": 42, "cluster_id": "11833932512235803003", "year": "2011", "url_citations": "http://scholar.google.com/scholar?cites=11833932512235803003&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 8, "url_citation": null, "title": "On parallelizability of stochastic gradient descent for speech dnns", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6853593", "url_versions": "http://scholar.google.com/scholar?cluster=17136515451612642598&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "F Seide, H Fu, J Droppo, G Li\u2026 - Acoustics, Speech and  \u2026", "excerpt": "ABSTRACT This paper compares the theoretical efficiency of model-parallel and data-parallel distributed stochastic gradient descent training of DNNs. For a typical Switchboard DNN with 46M parameters, the results are not pretty: With modern GPUs and  ...", "url_pdf": null, "num_citations": 15, "cluster_id": "17136515451612642598", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=17136515451612642598&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 5, "url_citation": null, "title": "Where do features come from?", "url": "http://onlinelibrary.wiley.com/doi/10.1111/cogs.12049/full", "url_versions": "http://scholar.google.com/scholar?cluster=1977854380276024110&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "G Hinton - Cognitive science", "excerpt": "Abstract It is possible to learn multiple layers of non-linear features by backpropagating error derivatives through a feedforward neural network. This is a very effective learning procedure when there is a huge amount of labeled training data, but for many learning tasks very few  ...", "url_pdf": null, "num_citations": 10, "cluster_id": "1977854380276024110", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=1977854380276024110&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 19, "url_citation": null, "title": "Krylov subspace descent for deep learning", "url": "http://arxiv.org/abs/1111.4259", "url_versions": "http://scholar.google.com/scholar?cluster=9397394098242250506&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "O Vinyals, D Povey - arXiv preprint arXiv:1111.4259", "excerpt": "Abstract: In this paper, we propose a second order optimization method to learn models where both the dimensionality of the parameter space and the number of training samples is high. In our method, we construct on each iteration a Krylov subspace formed by the  ...", "url_pdf": null, "num_citations": 33, "cluster_id": "9397394098242250506", "year": "2011", "url_citations": "http://scholar.google.com/scholar?cites=9397394098242250506&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 6, "url_citation": null, "title": "Big data deep learning: Challenges and perspectives", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6817512", "url_versions": "http://scholar.google.com/scholar?cluster=3358700578142176214&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "XW Chen, X Lin - Access, IEEE", "excerpt": "Abstract\u2014Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language  ...", "url_pdf": null, "num_citations": 18, "cluster_id": "3358700578142176214", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=3358700578142176214&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 2, "url_citation": null, "title": "Training and analysing deep recurrent neural networks", "url": "http://papers.nips.cc/paper/5166-tra", "url_versions": "http://scholar.google.com/scholar?cluster=14506396899502432477&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "M Hermans, B Schrauwen - Advances in Neural Information  \u2026", "excerpt": "Abstract Time series often have a temporal hierarchy, with information that is spread out over multiple time scales. Common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training  ...", "url_pdf": null, "num_citations": 35, "cluster_id": "14506396899502432477", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=14506396899502432477&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 3, "url_citation": null, "title": "Big neural networks waste capacity", "url": "http://arxiv.org/abs/1301.3583", "url_versions": "http://scholar.google.com/scholar?cluster=15203864254754138120&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "YN Dauphin, Y Bengio - arXiv preprint arXiv:1301.3583", "excerpt": "Abstract: This article exposes the failure of some big neural networks to leverage added capacity to reduce underfitting. Past research suggest diminishing returns when increasing the size of neural networks. Our experiments on ImageNet LSVRC-2010 show that this  ...", "url_pdf": null, "num_citations": 26, "cluster_id": "15203864254754138120", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=15203864254754138120&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 4, "url_citation": null, "title": "Training deep and recurrent networks with hessian-free optimization", "url": "http://link.springer.com/chapter/10.1007/978-3-642-35289-8_27", "url_versions": "http://scholar.google.com/scholar?cluster=3256736991833741484&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "J Martens, I Sutskever - Neural Networks: Tricks of the Trade", "excerpt": "Abstract In this chapter we will first describe the basic HF approach, and then examine well-known performance-improving techniques such as preconditioning which we have found to be beneficial for neural network training, as well as others of a more heuristic nature which  ...", "url_pdf": null, "num_citations": 34, "cluster_id": "3256736991833741484", "year": "2012", "url_citations": "http://scholar.google.com/scholar?cites=3256736991833741484&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 10, "url_citation": null, "title": "Investigations on hessian-free optimization for cross-entropy training of deep neural networks.", "url": "http://www.msr-waypoint.net/pubs/201363/is2013_wiesler.final.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=3815116398502475736&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "S Wiesler, J Li, J Xue - INTERSPEECH", "excerpt": "Abstract Context-dependent deep neural network HMMs have been shown to achieve recognition accuracy superior to Gaussian mixture models in a number of recent works. Typically, neural networks are optimized with stochastic gradient descent. On large  ...", "url_pdf": "http://www.msr-waypoint.net/pubs/201363/is2013_wiesler.final.pdf", "num_citations": 8, "cluster_id": "3815116398502475736", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=3815116398502475736&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 15, "url_citation": null, "title": "Sample size selection in optimization methods for machine learning", "url": "http://link.springer.com/article/10.1007/s10107-012-0572-5", "url_versions": "http://scholar.google.com/scholar?cluster=14083487002862439746&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "RH Byrd, GM Chin, J Nocedal, Y Wu - Mathematical programming", "excerpt": "Abstract This paper presents a methodology for using varying sample sizes in batch-type optimization methods for large-scale machine learning problems. The first part of the paper deals with the delicate issue of dynamic sample selection in the evaluation of the function  ...", "url_pdf": null, "num_citations": 44, "cluster_id": "14083487002862439746", "year": "2012", "url_citations": "http://scholar.google.com/scholar?cites=14083487002862439746&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 7, "url_citation": null, "title": "An overview of deep-structured learning for information processing", "url": "http://131.107.65.14/pubs/155609/DENG-APSIPA.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=1255369059227179273&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "L Deng - Proceedings of Asian-Pacific Signal & Information  \u2026", "excerpt": "Abstract\u2014In this paper, I will introduce to the APSIPA audience an emerging area of machine learning, deep-structured learning. It refers to a class of machine learning techniques, developed mostly since 2006, where many layers of information processing  ...", "url_pdf": "http://131.107.65.14/pubs/155609/DENG-APSIPA.pdf", "num_citations": 22, "cluster_id": "1255369059227179273", "year": "2011", "url_citations": "http://scholar.google.com/scholar?cites=1255369059227179273&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 3, "url_citation": null, "title": "A tutorial survey of architectures, algorithms, and applications for deep learning", "url": "http://journals.cambridge.org/abstract_S2048770313000097", "url_versions": "http://scholar.google.com/scholar?cluster=4381743472348703222&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "L Deng - APSIPA Transactions on Signal and Information  \u2026", "excerpt": "Abstract In this invited paper, my overview material on the same topic as presented in the plenary overview session of APSIPA-2011 and the tutorial material presented in the same conference [1] are expanded and updated to include more recent developments in deep  ...", "url_pdf": null, "num_citations": 18, "cluster_id": "4381743472348703222", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=4381743472348703222&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 12, "url_citation": null, "title": "Error back propagation for sequence training of context-dependent deep networks for conversational speech transcription", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6638951", "url_versions": "http://scholar.google.com/scholar?cluster=4487452001304092526&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "H Su, G Li, D Yu, F Seide - Acoustics, Speech and Signal  \u2026", "excerpt": "ABSTRACT We investigate back-propagation based sequence training of Context-Dependent Deep-Neural-Network HMMs, or CDDNN-HMMs, for conversational speech transcription. Theoretically, sequence training integrates with backpropagation in a  ...", "url_pdf": null, "num_citations": 60, "cluster_id": "4487452001304092526", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=4487452001304092526&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 17, "url_citation": null, "title": "The neural autoregressive distribution estimator", "url": "http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_LarochelleM11.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=2309248672502861649&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "H Larochelle, I Murray - International Conference on  \u2026", "excerpt": "Abstract We describe a new approach for modeling the distribution of high-dimensional vectors of discrete variables. This model is inspired by the restricted Boltzmann machine (RBM), which has been shown to be a powerful model of such distributions. However, an  ...", "url_pdf": "http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_LarochelleM11.pdf", "num_citations": 50, "cluster_id": "2309248672502861649", "year": "2011", "url_citations": "http://scholar.google.com/scholar?cites=2309248672502861649&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 10, "url_citation": null, "title": "A high-performance Cantonese keyword search system", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6639279", "url_versions": "http://scholar.google.com/scholar?cluster=10253249838113049262&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "B Kingsbury, J Cui, X Cui, MJF Gales\u2026 - \u2026 , Speech and Signal  \u2026", "excerpt": "ABSTRACT We present a system for keyword search on Cantonese conversational telephony audio, collected for the IARPA Babel program, that achieves good performance by combining postings lists produced by diverse speech recognition systems from three  ...", "url_pdf": null, "num_citations": 29, "cluster_id": "10253249838113049262", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=10253249838113049262&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 5, "url_citation": null, "title": "Deep learning for signal and information processing", "url": "http://cs.tju.edu.cn/web/docs/2013-Deep%20Learning%20for%20Signal%20and%20Information%20Processing.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=7346768574939973182&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "L Deng, D Yu - Microsoft Research Monograph", "excerpt": "ABSTRACT This short monograph contains the material expanded from two tutorials that the authors gave, one at APSIPA in October 2011 and the other at ICASSP in March 2012. Substantial updates have been made based on the literature up to March, 2013, covering  ...", "url_pdf": "http://cs.tju.edu.cn/web/docs/2013-Deep%20Learning%20for%20Signal%20and%20Information%20Processing.pdf", "num_citations": 9, "cluster_id": "7346768574939973182", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=7346768574939973182&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 4, "url_citation": null, "title": "How to construct deep recurrent neural networks", "url": "http://arxiv.org/abs/1312.6026", "url_versions": "http://scholar.google.com/scholar?cluster=3032210598922213468&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "R Pascanu, C Gulcehre, K Cho, Y Bengio - arXiv preprint arXiv:1312.6026", "excerpt": "Abstract: In this paper, we explore different ways to extend a recurrent neural network (RNN) to a\\ textit {deep} RNN. We start by arguing that the concept of depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the  ...", "url_pdf": null, "num_citations": 56, "cluster_id": "3032210598922213468", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=3032210598922213468&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 15, "url_citation": null, "title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5740583", "url_versions": "http://scholar.google.com/scholar?cluster=1536831630272977838&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "GE Dahl, D Yu, L Deng, A Acero - Audio, Speech, and  \u2026", "excerpt": "Abstract\u2014We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN- ...", "url_pdf": null, "num_citations": 739, "cluster_id": "1536831630272977838", "year": "2012", "url_citations": "http://scholar.google.com/scholar?cites=1536831630272977838&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 15, "url_citation": null, "title": "Deep learning made easier by linear transformations in perceptrons", "url": "http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2012_RaikoVL12.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=623608584055994978&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "T Raiko, H Valpola, Y LeCun - International  \u2026", "excerpt": "Abstract We transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. This transformation aims at separating the  ...", "url_pdf": "http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2012_RaikoVL12.pdf", "num_citations": 44, "cluster_id": "623608584055994978", "year": "2012", "url_citations": "http://scholar.google.com/scholar?cites=623608584055994978&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 33, "url_citation": null, "title": "Representation learning: A review and new perspectives", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6472238", "url_versions": "http://scholar.google.com/scholar?cluster=559463397382443088&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "Y Bengio, A Courville, P Vincent - Pattern Analysis and  \u2026", "excerpt": "Abstract\u2014The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the  ...", "url_pdf": null, "num_citations": 685, "cluster_id": "559463397382443088", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=559463397382443088&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 8, "url_citation": null, "title": "Learning recurrent neural networks with hessian-free optimization", "url": "http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Martens_532.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=4711861245001297265&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "J Martens, I Sutskever - Proceedings of the 28th  \u2026", "excerpt": "Abstract In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian ...", "url_pdf": "http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Martens_532.pdf", "num_citations": 163, "cluster_id": "4711861245001297265", "year": "2011", "url_citations": "http://scholar.google.com/scholar?cites=4711861245001297265&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 8, "url_citation": null, "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "url": "http://papers.nips.cc/paper/5486-sparse-pca-via-covariance-thresholding", "url_versions": "http://scholar.google.com/scholar?cluster=12321526113830855618&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "YN Dauphin, R Pascanu, C Gulcehre, K Cho\u2026 - Advances in Neural  \u2026", "excerpt": "Abstract A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it  ...", "url_pdf": null, "num_citations": 32, "cluster_id": "12321526113830855618", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=12321526113830855618&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 14, "url_citation": null, "title": "Generating text with recurrent neural networks", "url": "http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=14239073725713654823&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "I Sutskever, J Martens\u2026 - Proceedings of the  \u2026", "excerpt": "Abstract Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties  ...", "url_pdf": "http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf", "num_citations": 141, "cluster_id": "14239073725713654823", "year": "2011", "url_citations": "http://scholar.google.com/scholar?cites=14239073725713654823&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 5, "url_citation": null, "title": "Improved preconditioner for hessian free optimization", "url": "http://www.chapelle.cc/olivier/pub/precond.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=4369584637674824037&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "O Chapelle, D Erhan - NIPS Workshop on Deep Learning and  \u2026", "excerpt": "Abstract We investigate the use of Hessian Free optimization for learning deep autoencoders. One of the critical components in that algorithm is the choice of the preconditioner. We argue in this paper that the Jacobi preconditioner leads to faster  ...", "url_pdf": "http://www.chapelle.cc/olivier/pub/precond.pdf", "num_citations": 15, "cluster_id": "4369584637674824037", "year": "2011", "url_citations": "http://scholar.google.com/scholar?cites=4369584637674824037&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 0, "url_citation": null, "title": "Exploiting diversity for spoken term detection", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6639280", "url_versions": null, "authors": "L Mangu, H Soltau, HK Kuo\u2026 - \u2026 , Speech and Signal  \u2026", "excerpt": "ABSTRACT The paper describes a state-of-the-art spoken term detection system in which significant improvements are obtained by diversifying the ASR engines used for indexing and combining the search results. First, we describe the design factors that, when varied,  ...", "url_pdf": null, "num_citations": 33, "cluster_id": "1677325262704499577", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=1677325262704499577&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 4, "url_citation": null, "title": "Advances in optimizing recurrent networks", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6639349", "url_versions": "http://scholar.google.com/scholar?cluster=56668020054253477&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "Y Bengio, N Boulanger-Lewandowski\u2026 - \u2026 , Speech and Signal  \u2026", "excerpt": "ABSTRACT After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions  ...", "url_pdf": null, "num_citations": 54, "cluster_id": "56668020054253477", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=56668020054253477&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 6, "url_citation": null, "title": "Practical recommendations for gradient-based training of deep architectures", "url": "http://link.springer.com/chapter/10.1007/978-3-642-35289-8_26", "url_versions": "http://scholar.google.com/scholar?cluster=13214514639523956782&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "Y Bengio - Neural Networks: Tricks of the Trade", "excerpt": "Abstract Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most  ...", "url_pdf": null, "num_citations": 91, "cluster_id": "13214514639523956782", "year": "2012", "url_citations": "http://scholar.google.com/scholar?cites=13214514639523956782&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 27, "url_citation": null, "title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6296526", "url_versions": "http://scholar.google.com/scholar?cluster=3674494786452480182&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "G Hinton, L Deng, D Yu, GE Dahl\u2026 - Signal Processing  \u2026", "excerpt": "Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes  ...", "url_pdf": null, "num_citations": 1010, "cluster_id": "3674494786452480182", "year": "2012", "url_citations": "http://scholar.google.com/scholar?cites=3674494786452480182&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 5, "url_citation": null, "title": "Training recurrent neural networks", "url": "http://www.cs.utoronto.ca/%7Eilya/pubs/ilya_sutskever_phd_thesis.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=11547556497378421036&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "I Sutskever -", "excerpt": "Recurrent Neural Networks (RNNs) are artificial neural network models that are well-suited for pattern classification tasks whose inputs and outputs are sequences. The importance of developing methods for mapping sequences to sequences is exemplified by tasks such as  ...", "url_pdf": "http://www.cs.utoronto.ca/%7Eilya/pubs/ilya_sutskever_phd_thesis.pdf", "num_citations": 43, "cluster_id": "11547556497378421036", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=11547556497378421036&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 3, "url_citation": null, "title": "Developing speech recognition systems for corpus indexing under the IARPA Babel program", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6638969", "url_versions": "http://scholar.google.com/scholar?cluster=917523068601258619&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "J Cui, X Cui, B Ramabhadran, JH Kim\u2026 - \u2026 , Speech and Signal  \u2026", "excerpt": "ABSTRACT Automatic speech recognition is a core component of many applications, including keyword search. In this paper we describe experiments on acoustic modeling, language modeling, and decoding for keyword search on a Cantonese conversational  ...", "url_pdf": null, "num_citations": 24, "cluster_id": "917523068601258619", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=917523068601258619&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 15, "url_citation": null, "title": "Large scale distributed deep networks", "url": "http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks", "url_versions": "http://scholar.google.com/scholar?cluster=9220704513857531974&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "J Dean, G Corrado, R Monga, K Chen\u2026 - Advances in Neural  \u2026", "excerpt": "Abstract Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of  ...", "url_pdf": null, "num_citations": 325, "cluster_id": "9220704513857531974", "year": "2012", "url_citations": "http://scholar.google.com/scholar?cites=9220704513857531974&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 5, "url_citation": null, "title": "Mean-normalized stochastic gradient for large-scale deep learning", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6853582", "url_versions": "http://scholar.google.com/scholar?cluster=856138149399481455&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "S Wiesler, A Richard, R Schluter\u2026 - Acoustics, Speech and  \u2026", "excerpt": "ABSTRACT Deep neural networks are typically optimized with stochastic gradient descent (SGD). In this work, we propose a novel second-order stochastic optimization algorithm. The algorithm is based on analytic results showing that a non-zero mean of features is harmful  ...", "url_pdf": null, "num_citations": 13, "cluster_id": "856138149399481455", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=856138149399481455&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 7, "url_citation": null, "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "url": "http://arxiv.org/abs/1312.6120", "url_versions": "http://scholar.google.com/scholar?cluster=9090095758382098911&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "AM Saxe, JL McClelland, S Ganguli - arXiv preprint arXiv:1312.6120", "excerpt": "Abstract: Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by  ...", "url_pdf": null, "num_citations": 31, "cluster_id": "9090095758382098911", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=9090095758382098911&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 4, "url_citation": null, "title": "Improved learning algorithms for restricted Boltzmann machines", "url": "http://scholar.google.com/https://aaltodoc.aalto.fi/handle/123456789/3666", "url_versions": "http://scholar.google.com/scholar?cluster=4800062953939870407&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "KH Cho -", "excerpt": "A restricted Boltzmann machine (RBM) is often used as a building block for constructing deep neural networks and deep generative models which have gained popularity recently as one way to learn complex and large probabilistic models. In these deep models, it is  ...", "url_pdf": null, "num_citations": 14, "cluster_id": "4800062953939870407", "year": "2011", "url_citations": "http://scholar.google.com/scholar?cites=4800062953939870407&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 5, "url_citation": null, "title": "Improvements to deep convolutional neural networks for LVCSR", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6707749", "url_versions": "http://scholar.google.com/scholar?cluster=2619460338086592320&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "TN Sainath, B Kingsbury, A Mohamed\u2026 - \u2026  (ASRU)", "excerpt": "ABSTRACT Deep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural Networks (DNN), as they are able to better reduce spectral variation in the input signal. This has also been confirmed experimentally, with CNNs showing improvements in  ...", "url_pdf": null, "num_citations": 50, "cluster_id": "2619460338086592320", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=2619460338086592320&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 12, "url_citation": null, "title": "Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks", "url": "http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00409", "url_versions": "http://scholar.google.com/scholar?cluster=12134602513335462563&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "D Sussillo, O Barak - Neural computation", "excerpt": "Recurrent neural networks (RNNs) are useful tools for learning nonlinear relationships between time-varying inputs and outputs with complex temporal dependencies. Recently developed algorithms have been successful at training RNNs to perform a wide variety of  ...", "url_pdf": null, "num_citations": 21, "cluster_id": "12134602513335462563", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=12134602513335462563&as_sdt=2005&sciodt=1,5&hl=en"}]