[{"num_versions": 3, "url_citation": null, "title": "Qualitatively characterizing neural network optimization problems", "url": "http://arxiv.org/abs/1412.6544", "url_versions": "http://scholar.google.com/scholar?cluster=11309177599523101036&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "IJ Goodfellow, O Vinyals - arXiv preprint arXiv:1412.6544", "excerpt": "Abstract: Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as  ...", "url_pdf": null, "num_citations": 0, "cluster_id": null, "year": "2014", "url_citations": null}, {"num_versions": 8, "url_citation": null, "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "url": "http://papers.nips.cc/paper/5486-sparse-pca-via-covariance-thresholding", "url_versions": "http://scholar.google.com/scholar?cluster=12321526113830855618&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "YN Dauphin, R Pascanu, C Gulcehre, K Cho\u2026 - Advances in Neural  \u2026", "excerpt": "Abstract A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it  ...", "url_pdf": null, "num_citations": 32, "cluster_id": "12321526113830855618", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=12321526113830855618&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 11, "url_citation": null, "title": "Avoiding pathologies in very deep networks", "url": "http://arxiv.org/abs/1402.5836", "url_versions": "http://scholar.google.com/scholar?cluster=3368909721886951994&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "D Duvenaud, O Rippel, RP Adams\u2026 - arXiv preprint arXiv: \u2026", "excerpt": "Abstract: Choosing appropriate architectures and regularization strategies for deep networks is crucial to good predictive performance. To shed light on this problem, we analyze the analogous problem of constructing useful priors on compositions of functions. Specifically,  ...", "url_pdf": null, "num_citations": 1, "cluster_id": "3368909721886951994", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=3368909721886951994&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 0, "url_citation": null, "title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images", "url": "http://arxiv.org/abs/1506.07365", "url_versions": null, "authors": "M Watter, JT Springenberg, J Boedecker\u2026 - arXiv preprint arXiv: \u2026", "excerpt": "Abstract: We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image  ...", "url_pdf": null, "num_citations": 1, "cluster_id": "14464025381144196926", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=14464025381144196926&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 8, "url_citation": null, "title": "Learning to Discover Efficient Mathematical Identities", "url": "http://papers.nips.cc/paper/5350-learning-to-discover-efficient-mathematical-identities", "url_versions": "http://scholar.google.com/scholar?cluster=3527123610419576702&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "W Zaremba, K Kurach, R Fergus - Advances in Neural Information  \u2026", "excerpt": "Abstract In this paper we explore how machine learning techniques can be applied to the discovery of efficient mathematical identities. We introduce an attribute grammar framework for representing symbolic expressions. Given a grammar of math operators, we build trees  ...", "url_pdf": null, "num_citations": 3, "cluster_id": "3527123610419576702", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=3527123610419576702&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 7, "url_citation": null, "title": "Very deep convolutional networks for large-scale image recognition", "url": "http://arxiv.org/abs/1409.1556", "url_versions": "http://scholar.google.com/scholar?cluster=15993525775437884507&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "K Simonyan, A Zisserman - arXiv preprint arXiv:1409.1556", "excerpt": "Abstract: In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on  ...", "url_pdf": null, "num_citations": 294, "cluster_id": "15993525775437884507", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=15993525775437884507&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 3, "url_citation": null, "title": "Random Walks: Training Very Deep Nonlinear Feed-Forward Networks with Smart Initialization", "url": "http://arxiv.org/abs/1412.6558", "url_versions": "http://scholar.google.com/scholar?cluster=6298476691028665612&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "D Sussillo - arXiv preprint arXiv:1412.6558", "excerpt": "Abstract: Training very deep networks is an important open problem in machine learning. One of many difficulties is that the norm of the back-propagated gradient can grow or decay exponentially. Here we show that training very deep feed-forward networks (FFNs) is not  ...", "url_pdf": null, "num_citations": 1, "cluster_id": "6298476691028665612", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=6298476691028665612&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 4, "url_citation": null, "title": "On the saddle point problem for non-convex optimization", "url": "http://arxiv.org/abs/1405.4604", "url_versions": "http://scholar.google.com/scholar?cluster=13610122826716134117&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "R Pascanu, YN Dauphin, S Ganguli\u2026 - arXiv preprint arXiv: \u2026", "excerpt": "Abstract: A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it  ...", "url_pdf": null, "num_citations": 4, "cluster_id": "13610122826716134117", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=13610122826716134117&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 0, "url_citation": null, "title": "Beyond Temporal Pooling: Recurrence and Temporal Convolutions for Gesture Recognition in Video", "url": "http://arxiv.org/abs/1506.01911", "url_versions": null, "authors": "L Pigou, A Oord, S Dieleman\u2026 - arXiv preprint arXiv: \u2026", "excerpt": "Abstract: Recent studies have demonstrated the power of recurrent neural networks for machine translation, image captioning and speech recognition. For the task of capturing temporal structure in video, however, there still remain numerous open research  ...", "url_pdf": null, "num_citations": 1, "cluster_id": "13545994664989830133", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=13545994664989830133&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 3, "url_citation": null, "title": "Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?", "url": "http://arxiv.org/abs/1504.08291", "url_versions": "http://scholar.google.com/scholar?cluster=7561728613456500534&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "R Giryes, G Sapiro, AM Bronstein - arXiv preprint arXiv:1504.08291", "excerpt": "Abstract: Two important properties of a classification machinery are:(i) the system preserves the important information of the input data;(ii) the training examples convey information for unseen data; and (iii) the system is able to treat differently points from different classes. In  ...", "url_pdf": null, "num_citations": 1, "cluster_id": "7561728613456500534", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=7561728613456500534&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 4, "url_citation": null, "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "url": "http://arxiv.org/abs/1502.01852", "url_versions": "http://scholar.google.com/scholar?cluster=6243061688889140249&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "K He, X Zhang, S Ren, J Sun - arXiv preprint arXiv:1502.01852", "excerpt": "Abstract: Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the  ...", "url_pdf": null, "num_citations": 73, "cluster_id": "6243061688889140249", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=6243061688889140249&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 5, "url_citation": null, "title": "The potential energy of an autoencoder", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6918504", "url_versions": "http://scholar.google.com/scholar?cluster=8349234916097051724&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "H Kamyshanska, R Memisevic -", "excerpt": "Abstract\u2014Autoencoders are popular feature learning models, that are conceptually simple, easy to train and allow for efficient inference. Recent work has shown how certain autoencoders can be associated with an energy landscape, akin to negative log- ...", "url_pdf": null, "num_citations": 3, "cluster_id": "8349234916097051724", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=8349234916097051724&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 8, "url_citation": null, "title": "On simplicity and complexity in the brave new world of large-scale neuroscience", "url": "http://www.sciencedirect.com/science/article/pii/S0959438815000768", "url_versions": "http://scholar.google.com/scholar?cluster=18414504014639043304&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "P Gao, S Ganguli - Current opinion in neurobiology", "excerpt": "ScienceDirect is phasing out support for older versions of Internet Explorer on Jan 12, 2016. For the best product experience, we recommend you upgrade to a newer version of IE or use a different browser: Firefox or Chrome. For additional information please see the  ...", "url_pdf": null, "num_citations": 2, "cluster_id": "18414504014639043304", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=18414504014639043304&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 0, "url_citation": null, "title": "Skip-thought vectors", "url": "http://arxiv.org/abs/1506.06726", "url_versions": null, "authors": "R Kiros, Y Zhu, R Salakhutdinov, RS Zemel\u2026 - arXiv preprint arXiv: \u2026", "excerpt": "Abstract: We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage.  ...", "url_pdf": null, "num_citations": 2, "cluster_id": "10194299428367499234", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=10194299428367499234&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 11, "url_citation": null, "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "url": "http://arxiv.org/abs/1406.1078", "url_versions": "http://scholar.google.com/scholar?cluster=9119975171114587835&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "K Cho, B Van Merri\u00ebnboer, C Gulcehre\u2026 - arXiv preprint arXiv: \u2026", "excerpt": "Abstract: In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the  ...", "url_pdf": null, "num_citations": 113, "cluster_id": "9119975171114587835", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=9119975171114587835&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 2, "url_citation": null, "title": "Escaping From Saddle Points---Online Stochastic Gradient for Tensor Decomposition", "url": "http://arxiv.org/abs/1503.02101", "url_versions": "http://scholar.google.com/scholar?cluster=8145096536293957391&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "R Ge, F Huang, C Jin, Y Yuan - arXiv preprint arXiv:1503.02101", "excerpt": "Abstract: We analyze stochastic gradient descent for optimizing non-convex functions. In many cases for non-convex functions the goal is to find a reasonable local minimum, and the main concern is that gradient updates are trapped in saddle points. In this paper we  ...", "url_pdf": null, "num_citations": 2, "cluster_id": "8145096536293957391", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=8145096536293957391&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 9, "url_citation": null, "title": "The loss surface of multilayer networks", "url": "http://arxiv.org/abs/1412.0233", "url_versions": "http://scholar.google.com/scholar?cluster=338235050395741201&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "A Choromanska, M Henaff, M Mathieu\u2026 - arXiv preprint arXiv: \u2026", "excerpt": "Abstract: We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii)  ...", "url_pdf": null, "num_citations": 13, "cluster_id": "338235050395741201", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=338235050395741201&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 3, "url_citation": null, "title": "Pre-training of Recurrent Neural Networks via Linear Autoencoders", "url": "http://papers.nips.cc/paper/5271-pre-training-of-recurrent-neural-networks-via-linear-autoencoders", "url_versions": "http://scholar.google.com/scholar?cluster=11703165863807314736&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "L Pasa, A Sperduti - Advances in Neural Information Processing  \u2026", "excerpt": "Abstract We propose a pre-training technique for recurrent neural networks based on linear autoencoder networks for sequences, ie linear dynamical systems modelling the target sequences. We start by giving a closed form solution for the definition of the optimal  ...", "url_pdf": null, "num_citations": 3, "cluster_id": "11703165863807314736", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=11703165863807314736&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 11, "url_citation": null, "title": "Learning polynomials with neural networks", "url": "http://machinelearning.wustl.edu/mlpapers/papers/icml2014c2_andoni14", "url_versions": "http://scholar.google.com/scholar?cluster=10614318723724968087&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "A Andoni, R Panigrahy\u2026 - Proceedings of the  \u2026", "excerpt": "Abstract: We study the effectiveness of learning low degree polynomials using neural networks by the gradient descent method. While neural networks have been shown to have great expressive power, and gradient descent has been widely used in practice for  ...", "url_pdf": null, "num_citations": 6, "cluster_id": "10614318723724968087", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=10614318723724968087&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 2, "url_citation": null, "title": "A Linear Dynamical System Model for Text", "url": "http://arxiv.org/abs/1502.04081", "url_versions": "http://scholar.google.com/scholar?cluster=2467213344437448012&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "D Belanger, S Kakade - arXiv preprint arXiv:1502.04081", "excerpt": "Abstract: Low dimensional representations of words allow accurate NLP models to be trained on limited annotated data. While most representations ignore words' local context, a natural way to induce context-dependent representations is to perform inference in a  ...", "url_pdf": null, "num_citations": 1, "cluster_id": "2467213344437448012", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=2467213344437448012&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 6, "url_citation": null, "title": "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units", "url": "http://arxiv.org/abs/1504.00941", "url_versions": "http://scholar.google.com/scholar?cluster=13158688352232587853&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "QV Le, N Jaitly, GE Hinton - arXiv preprint arXiv:1504.00941", "excerpt": "Abstract: Learning long term dependencies in recurrent networks is difficult due to vanishing and exploding gradients. To overcome this difficulty, researchers have developed sophisticated optimization techniques and network architectures. In this paper, we  ...", "url_pdf": null, "num_citations": 2, "cluster_id": "13158688352232587853", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=13158688352232587853&as_sdt=2005&sciodt=1,5&hl=en"}]