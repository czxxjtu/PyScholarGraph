[{"num_versions": 0, "url_citation": null, "title": "Path-SGD: Path-Normalized Optimization in Deep Neural Networks", "url": "http://arxiv.org/abs/1506.02617", "url_versions": null, "authors": "B Neyshabur, R Salakhutdinov, N Srebro - arXiv preprint arXiv: \u2026", "excerpt": "Abstract: We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD,  ...", "url_pdf": null, "num_citations": 0, "cluster_id": null, "year": "2015", "url_citations": null}, {"num_versions": 0, "url_citation": null, "title": "Train faster, generalize better: Stability of stochastic gradient descent", "url": "http://arxiv.org/abs/1509.01240", "url_versions": null, "authors": "M Hardt, B Recht, Y Singer - arXiv preprint arXiv:1509.01240", "excerpt": "Abstract: We show that any model trained by a stochastic gradient method with few iterations has vanishing generalization error. We prove this by showing the method is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools  ...", "url_pdf": null, "num_citations": 1, "cluster_id": "4563905800844853958", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=4563905800844853958&as_sdt=2005&sciodt=1,5&hl=en"}]