[{"num_versions": 0, "url_citation": null, "title": "Deep Online Convex Optimization by Putting Forecaster to Sleep", "url": "http://arxiv.org/abs/1509.01851", "url_versions": null, "authors": "D Balduzzi - arXiv preprint arXiv:1509.01851", "excerpt": "Abstract: Methods from convex optimization such as accelerated gradient descent are widely used as building blocks for deep learning algorithms. However, the reasons for their empirical success are unclear, since neural networks are not convex and standard  ...", "url_pdf": null, "num_citations": 1, "cluster_id": "1937498321554206888", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=1937498321554206888&as_sdt=2005&sciodt=1,5&hl=en"}]