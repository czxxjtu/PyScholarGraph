[{"num_versions": 2, "url_citation": null, "title": "Modeling Spatial-Temporal Clues in a Hybrid Deep Learning Framework for Video Classification", "url": "http://arxiv.org/abs/1504.01561", "url_versions": "http://scholar.google.com/scholar?cluster=15677418431280773725&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "Z Wu, X Wang, YG Jiang, H Ye, X Xue - arXiv preprint arXiv:1504.01561", "excerpt": "Abstract: Classifying videos according to content semantics is an important problem with a wide range of applications. In this paper, we propose a hybrid deep learning framework for video classification, which is able to model static spatial information, short-term motion, as  ...", "url_pdf": null, "num_citations": 3, "cluster_id": "15677418431280773725", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=15677418431280773725&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 2, "url_citation": null, "title": "Video description generation incorporating spatio-temporal features and a soft-attention mechanism", "url": "http://arxiv.org/abs/1502.08029", "url_versions": "http://scholar.google.com/scholar?cluster=8761564837485893612&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "L Yao, A Torabi, K Cho, N Ballas, C Pal\u2026 - arXiv preprint arXiv: \u2026", "excerpt": "Abstract: Recent progress in using recurrent neural networks (RNNs) for image description has motivated us to explore the application of RNNs to video description. Recent work has also suggested that attention mechanisms may be able to increase performance. To this  ...", "url_pdf": null, "num_citations": 9, "cluster_id": "8761564837485893612", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=8761564837485893612&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 0, "url_citation": null, "title": "From Deep Learning to Episodic Memories: Creating Categories of Visual Experiences", "url": "http://www.cogsys.org/papers/ACS2015/article15.pdf", "url_versions": null, "authors": "J Doshi, Z Kira, A Wagner - Proceedings of the Third Annual Conference  \u2026", "excerpt": "Abstract This paper presents a cognitively inspired approach for visual scene categorization and abstraction. Our approach uses first-person video from real, dynamic environments to create episode-like memories of video scenes. Videos from newly encountered  ...", "url_pdf": "http://www.cogsys.org/papers/ACS2015/article15.pdf", "num_citations": 0, "cluster_id": null, "year": "2015", "url_citations": null}, {"num_versions": 0, "url_citation": null, "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "url": "http://arxiv.org/abs/1506.06724", "url_versions": null, "authors": "Y Zhu, R Kiros, R Zemel, R Salakhutdinov\u2026 - arXiv preprint arXiv: \u2026", "excerpt": "Abstract: Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie  ...", "url_pdf": null, "num_citations": 1, "cluster_id": "4414474086145090761", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=4414474086145090761&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 0, "url_citation": null, "title": "Visual Madlibs: Fill in the blank Image Generation and Question Answering", "url": "http://arxiv.org/abs/1506.00278", "url_versions": null, "authors": "L Yu, E Park, AC Berg, TL Berg - arXiv preprint arXiv:1506.00278", "excerpt": "Abstract: In this paper, we introduce a new dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather  ...", "url_pdf": null, "num_citations": 1, "cluster_id": "1060109126307380715", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=1060109126307380715&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 0, "url_citation": null, "title": "Describing videos by exploiting temporal structure", "url": "http://ttic.uchicago.edu/%7Ehaotang/speech/1502.08029v4.pdf", "url_versions": null, "authors": "L Yao, A Torabi, K Cho, N Ballas, C Pal, H Larochelle\u2026 - stat", "excerpt": "Abstract Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure  ...", "url_pdf": "http://ttic.uchicago.edu/%7Ehaotang/speech/1502.08029v4.pdf", "num_citations": 5, "cluster_id": "17225606232504528023", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=17225606232504528023&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 4, "url_citation": null, "title": "Phrase-based image captioning", "url": "http://arxiv.org/abs/1502.03671", "url_versions": "http://scholar.google.com/scholar?cluster=9868419265152877728&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "R Lebret, PO Pinheiro, R Collobert - arXiv preprint arXiv:1502.03671", "excerpt": "Abstract: Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This  ...", "url_pdf": null, "num_citations": 6, "cluster_id": "9868419265152877728", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=9868419265152877728&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 2, "url_citation": null, "title": "Jointly Modeling Embedding and Translation to Bridge Video and Language", "url": "http://arxiv.org/abs/1505.01861", "url_versions": "http://scholar.google.com/scholar?cluster=5029588966573808727&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "Y Pan, T Mei, T Yao, H Li, Y Rui - arXiv preprint arXiv:1505.01861", "excerpt": "Abstract: Automatically describing video content with natural language is a fundamental challenge of multimedia. Recurrent Neural Networks (RNN), which models sequence dynamics, has attracted increasing attention on visual interpretation. However, most  ...", "url_pdf": null, "num_citations": 1, "cluster_id": "5029588966573808727", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=5029588966573808727&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 0, "url_citation": null, "title": "Learning language through pictures", "url": "http://arxiv.org/abs/1506.03694", "url_versions": null, "authors": "G Chrupa\u0142a, A K\u00e1d\u00e1r, A Alishahi - arXiv preprint arXiv:1506.03694", "excerpt": "Abstract: We propose Imaginet, a model of learning visually grounded representations of language from coupled textual and visual input. The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving  ...", "url_pdf": null, "num_citations": 3, "cluster_id": "2043966200281309899", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=2043966200281309899&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 3, "url_citation": null, "title": "Using descriptive video services to create a large data source for video annotation research", "url": "http://arxiv.org/abs/1503.01070", "url_versions": "http://scholar.google.com/scholar?cluster=13435611438272203025&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "A Torabi, C Pal, H Larochelle, A Courville - arXiv preprint arXiv: \u2026", "excerpt": "Abstract: In this work, we introduce a dataset of video annotated with high quality natural language phrases describing the visual content in a given segment of time. Our dataset is based on the Descriptive Video Service (DVS) that is now encoded on many digital media  ...", "url_pdf": null, "num_citations": 5, "cluster_id": "13435611438272203025", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=13435611438272203025&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 0, "url_citation": null, "title": "Describing Multimedia Content using Attention-based Encoder\u2013Decoder Networks", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7243334", "url_versions": null, "authors": "K Cho, A Courville, Y Bengio -", "excerpt": "Abstract\u2014Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input ...", "url_pdf": null, "num_citations": 1, "cluster_id": "12875719958826891977", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=12875719958826891977&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 3, "url_citation": null, "title": "Evaluating Two-Stream CNN for Video Classification", "url": "http://arxiv.org/abs/1504.01920", "url_versions": "http://scholar.google.com/scholar?cluster=4679610789887160570&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "H Ye, Z Wu, RW Zhao, X Wang, YG Jiang\u2026 - arXiv preprint arXiv: \u2026", "excerpt": "Abstract: Videos contain very rich semantic information. Traditional hand-crafted features are known to be inadequate in analyzing complex video semantics. Inspired by the huge success of the deep learning methods in analyzing image, audio and text data, significant  ...", "url_pdf": null, "num_citations": 1, "cluster_id": "4679610789887160570", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=4679610789887160570&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 8, "url_citation": null, "title": "A dataset for movie description", "url": "http://arxiv.org/abs/1501.02530", "url_versions": "http://scholar.google.com/scholar?cluster=7783484590158939750&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "A Rohrbach, M Rohrbach, N Tandon\u2026 - arXiv preprint arXiv: \u2026", "excerpt": "Abstract: Descriptive video service (DVS) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for  ...", "url_pdf": null, "num_citations": 10, "cluster_id": "7783484590158939750", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=7783484590158939750&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 3, "url_citation": null, "title": "Book2movie: Aligning video scenes with book chapters", "url": "http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/1B_078.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=14694155721082020146&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "M Tapaswi, M B\u00e4uml\u2026 - Proceedings of the IEEE  \u2026", "excerpt": "Abstract Film adaptations of novels often visually display in a few shots what is described in many pages of the source novel. In this paper we present a new problem: to align book chapters with video scenes. Such an alignment facilitates finding differences between the  ...", "url_pdf": "http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/1B_078.pdf", "num_citations": 2, "cluster_id": "14694155721082020146", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=14694155721082020146&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 3, "url_citation": null, "title": "Sequence to Sequence--Video to Text", "url": "http://arxiv.org/abs/1505.00487", "url_versions": "http://scholar.google.com/scholar?cluster=2341786742021115670&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "S Venugopalan, M Rohrbach, J Donahue\u2026 - arXiv preprint arXiv: \u2026", "excerpt": "Abstract: Real-world videos often have complex dynamics; methods for generating open-domain video descriptions should be senstive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this  ...", "url_pdf": null, "num_citations": 6, "cluster_id": "2341786742021115670", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=2341786742021115670&as_sdt=2005&sciodt=1,5&hl=en"}]