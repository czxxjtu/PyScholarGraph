[{"num_versions": 0, "url_citation": null, "title": "Tensorizing Neural Networks", "url": "http://arxiv.org/abs/1509.06569", "url_versions": null, "authors": "A Novikov, D Podoprikhin, A Osokin\u2026 - arXiv preprint arXiv: \u2026", "excerpt": "Abstract: Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly  ...", "url_pdf": null, "num_citations": 0, "cluster_id": null, "year": "2015", "url_citations": null}, {"num_versions": 0, "url_citation": null, "title": "Taming the Wild: A Unified Analysis of Hogwild!-Style Algorithms", "url": "http://arxiv.org/abs/1506.06438", "url_versions": null, "authors": "C De Sa, C Zhang, K Olukotun, C R\u00e9 - arXiv preprint arXiv:1506.06438", "excerpt": "Abstract: Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of machine learning problems. Researchers and industry have developed several techniques to optimize SGD's runtime performance, including asynchronous execution and reduced  ...", "url_pdf": null, "num_citations": 1, "cluster_id": "12804573388299887254", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=12804573388299887254&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 2, "url_citation": null, "title": "PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions", "url": "http://arxiv.org/abs/1504.08362", "url_versions": "http://scholar.google.com/scholar?cluster=555676852530582844&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "M Figurnov, D Vetrov, P Kohli - arXiv preprint arXiv:1504.08362", "excerpt": "Abstract: This paper proposes a novel approach to reduce the computational cost of evaluation of convolutional neural networks, a factor that has hindered their deployment in low-power devices such as mobile phones. Our method is inspired by the loop perforation  ...", "url_pdf": null, "num_citations": 1, "cluster_id": "555676852530582844", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=555676852530582844&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 3, "url_citation": null, "title": "Compressing Neural Networks with the Hashing Trick", "url": "http://arxiv.org/abs/1504.04788", "url_versions": "http://scholar.google.com/scholar?cluster=5053947540904220409&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "W Chen, JT Wilson, S Tyree, KQ Weinberger\u2026 - arXiv preprint arXiv: \u2026", "excerpt": "Abstract: As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very  ...", "url_pdf": null, "num_citations": 6, "cluster_id": "5053947540904220409", "year": "2015", "url_citations": "http://scholar.google.com/scholar?cites=5053947540904220409&as_sdt=2005&sciodt=1,5&hl=en"}]