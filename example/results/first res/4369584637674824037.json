[{"num_versions": 5, "url_citation": null, "title": "Training recurrent neural networks", "url": "http://www.cs.utoronto.ca/%7Eilya/pubs/ilya_sutskever_phd_thesis.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=11547556497378421036&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "year": "2013", "excerpt": "Recurrent Neural Networks (RNNs) are artificial neural network models that are well-suited for pattern classification tasks whose inputs and outputs are sequences. The importance of developing methods for mapping sequences to sequences is exemplified by tasks such as  ...", "url_pdf": "http://www.cs.utoronto.ca/%7Eilya/pubs/ilya_sutskever_phd_thesis.pdf", "num_citations": 43, "cluster_id": "11547556497378421036", "authors": "I Sutskever -", "url_citations": "http://scholar.google.com/scholar?cites=11547556497378421036&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 2, "url_citation": null, "title": "Beyond Deep Learning: Scalable Methods and Models for Learning", "url": "http://digitalassets.lib.berkeley.edu/techreports/ucb/text/EECS-2013-202.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=14734532495968495597&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "year": "2013", "excerpt": "In this thesis I present results from work that I hope will provide contributions to the speech, vision, and machine learning communities. As a result of working in these different areas, my thesis is interdisciplinary. The main emphasis is, however, common to all three: to learn  ...", "url_pdf": "http://digitalassets.lib.berkeley.edu/techreports/ucb/text/EECS-2013-202.pdf", "num_citations": 0, "cluster_id": null, "authors": "O Vinyals -", "url_citations": null}, {"num_versions": 3, "url_citation": null, "title": "Metric-free natural gradient for joint-training of boltzmann machines", "url": "http://arxiv.org/abs/1301.3545", "url_versions": "http://scholar.google.com/scholar?cluster=5058021713135303817&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "year": "2013", "excerpt": "Abstract: This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for training Boltzmann Machines. Similar in spirit to the Hessian-Free method of Martens [8], our algorithm belongs to the family of truncated Newton methods and exploits an efficient  ...", "url_pdf": null, "num_citations": 6, "cluster_id": "5058021713135303817", "authors": "G Desjardins, R Pascanu, A Courville\u2026 - arXiv preprint arXiv: \u2026", "url_citations": "http://scholar.google.com/scholar?cites=5058021713135303817&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 4, "url_citation": null, "title": "Revisiting natural gradient for deep networks", "url": "http://arxiv.org/abs/1301.3584", "url_versions": "http://scholar.google.com/scholar?cluster=4887476036427842105&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "year": "2013", "excerpt": "Abstract: We evaluate natural gradient, an algorithm originally proposed in Amari (1997), for learning deep models. The contributions of this paper are as follows. We show the connection between natural gradient and three other recently proposed methods for  ...", "url_pdf": null, "num_citations": 30, "cluster_id": "4887476036427842105", "authors": "R Pascanu, Y Bengio - arXiv preprint arXiv:1301.3584", "url_citations": "http://scholar.google.com/scholar?cites=4887476036427842105&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 12, "url_citation": null, "title": "Estimating the hessian by back-propagating curvature", "url": "http://arxiv.org/abs/1206.6464", "url_versions": "http://scholar.google.com/scholar?cluster=3689104787639440045&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "year": "2012", "excerpt": "Abstract: In this work we develop Curvature Propagation (CP), a general technique for efficiently computing unbiased approximations of the Hessian of any function that is computed using a computational graph. At the cost of roughly two gradient evaluations,  ...", "url_pdf": null, "num_citations": 5, "cluster_id": "3689104787639440045", "authors": "J Martens, I Sutskever, K Swersky - arXiv preprint arXiv:1206.6464", "url_citations": "http://scholar.google.com/scholar?cites=3689104787639440045&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 23, "url_citation": null, "title": "No more pesky learning rates", "url": "http://arxiv.org/abs/1206.1106", "url_versions": "http://scholar.google.com/scholar?cluster=4544789299729524763&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "year": "2012", "excerpt": "Abstract: The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The  ...", "url_pdf": null, "num_citations": 68, "cluster_id": "4544789299729524763", "authors": "T Schaul, S Zhang, Y LeCun - arXiv preprint arXiv:1206.1106", "url_citations": "http://scholar.google.com/scholar?cites=4544789299729524763&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 16, "url_citation": null, "title": "On the importance of initialization and momentum in deep learning", "url": "http://machinelearning.wustl.edu/mlpapers/papers/icml2013_sutskever13", "url_versions": "http://scholar.google.com/scholar?cluster=7449004388220998591&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "year": "2013", "excerpt": "Abstract: Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent  ...", "url_pdf": null, "num_citations": 154, "cluster_id": "7449004388220998591", "authors": "I Sutskever, J Martens, G Dahl\u2026 - Proceedings of the  \u2026", "url_citations": "http://scholar.google.com/scholar?cites=7449004388220998591&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 6, "url_citation": null, "title": "Accelerating Hessian-free optimization for deep neural networks by implicit preconditioning and sampling", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6707747", "url_versions": "http://scholar.google.com/scholar?cluster=2293470901273086567&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "year": "2013", "excerpt": "Hessian-free training has become a popular parallel second order optimization technique for Deep Neural Network training. This study aims at speeding up Hessian-free training, both by means of decreasing the amount of data used for training, as well as through reduction  ...", "url_pdf": null, "num_citations": 4, "cluster_id": "2293470901273086567", "authors": "TN Sainath, L Horesh, B Kingsbury\u2026 - \u2026  (ASRU)", "url_citations": "http://scholar.google.com/scholar?cites=2293470901273086567&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 7, "url_citation": null, "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "url": "http://arxiv.org/abs/1312.6120", "url_versions": "http://scholar.google.com/scholar?cluster=9090095758382098911&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "year": "2013", "excerpt": "Abstract: Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by  ...", "url_pdf": null, "num_citations": 31, "cluster_id": "9090095758382098911", "authors": "AM Saxe, JL McClelland, S Ganguli - arXiv preprint arXiv:1312.6120", "url_citations": "http://scholar.google.com/scholar?cites=9090095758382098911&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 4, "url_citation": null, "title": "Training deep and recurrent networks with hessian-free optimization", "url": "http://link.springer.com/chapter/10.1007/978-3-642-35289-8_27", "url_versions": "http://scholar.google.com/scholar?cluster=3256736991833741484&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "year": "2012", "excerpt": "Abstract In this chapter we will first describe the basic HF approach, and then examine well-known performance-improving techniques such as preconditioning which we have found to be beneficial for neural network training, as well as others of a more heuristic nature which  ...", "url_pdf": null, "num_citations": 34, "cluster_id": "3256736991833741484", "authors": "J Martens, I Sutskever - Neural Networks: Tricks of the Trade", "url_citations": "http://scholar.google.com/scholar?cites=3256736991833741484&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 6, "url_citation": null, "title": "Training neural networks with stochastic hessian-free optimization", "url": "http://arxiv.org/abs/1301.3641", "url_versions": "http://scholar.google.com/scholar?cluster=15998932101819720935&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "year": "2013", "excerpt": "Abstract: Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same  ...", "url_pdf": null, "num_citations": 5, "cluster_id": "15998932101819720935", "authors": "R Kiros - arXiv preprint arXiv:1301.3641", "url_citations": "http://scholar.google.com/scholar?cites=15998932101819720935&as_sdt=2005&sciodt=1,5&hl=en"}]