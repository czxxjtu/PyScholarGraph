[{"num_versions": 58, "url_citation": null, "title": "Reinforcement learning: An introduction", "url": "http://www.cell.com/trends/cognitive-sciences/pdf/S1364-6613%2899%2901331-5.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=8543918805023417520&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "RS Sutton, AG Barto -", "excerpt": "The present book is an excellent entry point for someone who wants to understand intuitively the ideas of reinforcement learning and the general connection between its parts. It is not, however, a mathematical 'how-to'book, replete with proofs and pointers to unsolved  ...", "url_pdf": "http://www.cell.com/trends/cognitive-sciences/pdf/S1364-6613%2899%2901331-5.pdf", "num_citations": 19746, "cluster_id": "8543918805023417520", "year": "1998", "url_citations": "http://scholar.google.com/scholar?cites=8543918805023417520&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 33, "url_citation": null, "title": "Recent advances in hierarchical reinforcement learning", "url": "http://link.springer.com/article/10.1023/A:1022140919877", "url_versions": "http://scholar.google.com/scholar?cluster=15487186943679931860&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "AG Barto, S Mahadevan - Discrete Event Dynamic Systems", "excerpt": "Abstract Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled  ...", "url_pdf": null, "num_citations": 661, "cluster_id": "15487186943679931860", "year": "2003", "url_citations": "http://scholar.google.com/scholar?cites=15487186943679931860&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 15, "url_citation": null, "title": "Elevator group control using multiple reinforcement learning agents", "url": "http://link.springer.com/article/10.1023/A:1007518724497", "url_versions": "http://scholar.google.com/scholar?cluster=10963221272726557928&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "RH Crites, AG Barto - Machine Learning", "excerpt": "Abstract Recent algorithmic and theoretical advances in reinforcement learning (RL) have attracted widespread interest. RL algorithms have appeared that approximate dynamic programming on an incremental basis. They can be trained on the basis of real or  ...", "url_pdf": null, "num_citations": 276, "cluster_id": "10963221272726557928", "year": "1998", "url_citations": "http://scholar.google.com/scholar?cites=10963221272726557928&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 8, "url_citation": null, "title": "Searching for solutions in games and artificial intelligence", "url": "http://www.aiexp.info/files/allis-thesis.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=16245287334395854781&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "LV Allis -", "excerpt": "The research presented in this thesis would have been impossible without the help of many persons, whom I want to recognize here. First of all, I would like to thank Jaap van den Herik for being my teacher. Jaap created an environment generously providing an abundance  ...", "url_pdf": "http://www.aiexp.info/files/allis-thesis.pdf", "num_citations": 404, "cluster_id": "16245287334395854781", "year": "1994", "url_citations": "http://scholar.google.com/scholar?cites=16245287334395854781&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 16, "url_citation": null, "title": "Experiments with infinite-horizon, policy-gradient estimation", "url": "http://www.jair.org/papers/paper807.html", "url_versions": "http://scholar.google.com/scholar?cluster=16853251126764402795&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "J Baxter, PL Bartlett, L Weaver - Journal of Artificial Intelligence Research", "excerpt": "Abstract In this paper, we present algorithms that perform gradient ascent of the average reward in a partially observable Markov decision process (POMDP). These algorithms are based on GPOMDP, an algorithm introduced in a companion paper (Baxter & Bartlett,  ...", "url_pdf": null, "num_citations": 158, "cluster_id": "16853251126764402795", "year": "2001", "url_citations": "http://scholar.google.com/scholar?cites=16853251126764402795&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 27, "url_citation": null, "title": "Texts in Theoretical Computer Science An EATCS Series", "url": "http://link.springer.com/content/pdf/10.1007/3-540-28520-2.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=1656538971777474347&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "GAMBCS Calude, ACDHJ Hartmanis, T Henzinger\u2026 -", "excerpt": "This book is an accessible introduction to complexity theory and cryptology, two closely related areas in theoretical computer science. Based on courses taught at Heinrich-Heine-Universit\u00e4t D\u00fcsseldorf and Friedrich-Schiller-Universit\u00e4t Jena since 1996, this textbook is  ...", "url_pdf": "http://link.springer.com/content/pdf/10.1007/3-540-28520-2.pdf", "num_citations": 623, "cluster_id": "1656538971777474347", "year": "2005", "url_citations": "http://scholar.google.com/scholar?cites=1656538971777474347&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 19, "url_citation": null, "title": "Dopamine neurons can represent context-dependent prediction error", "url": "http://www.sciencedirect.com/science/article/pii/S0896627303008699", "url_versions": "http://scholar.google.com/scholar?cluster=8528871066422252479&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "H Nakahara, H Itoh, R Kawagoe, Y Takikawa\u2026 - Neuron", "excerpt": "Midbrain dopamine (DA) neurons are thought to encode reward prediction error. Reward prediction can be improved if any relevant context is taken into account. We found that monkey DA neurons can encode a context-dependent prediction error. In the first  ...", "url_pdf": null, "num_citations": 184, "cluster_id": "8528871066422252479", "year": "2004", "url_citations": "http://scholar.google.com/scholar?cites=8528871066422252479&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 13, "url_citation": null, "title": "Information processing, dimensionality reduction and reinforcement learning in the basal ganglia", "url": "http://www.sciencedirect.com/science/article/pii/S0301008203001928", "url_versions": "http://scholar.google.com/scholar?cluster=15168094797285390910&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "I Bar-Gad, G Morris, H Bergman - Progress in neurobiology", "excerpt": "Modeling of the basal ganglia has played a major role in our understanding of this elusive group of nuclei. Models of the basal ganglia have undergone evolutionary and revolutionary changes over the last 20 years, as new research in the fields of anatomy, physiology and  ...", "url_pdf": null, "num_citations": 254, "cluster_id": "15168094797285390910", "year": "2003", "url_citations": "http://scholar.google.com/scholar?cites=15168094797285390910&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 6, "url_citation": null, "title": "Book Review: Reward Signaling by Dopamine Neurons", "url": "http://nro.sagepub.com/content/7/4/293.short", "url_versions": "http://scholar.google.com/scholar?cluster=14317077142483259655&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "W Schultz - The Neuroscientist", "excerpt": "Abstract Dopamine projections from the midbrain to the striatum and frontal cortex are involved in behavioral reactions controlled by rewards, as inferred from deficits in parkinsonism, schizophrenia, and drug addiction. Recent experiments have shown that  ...", "url_pdf": null, "num_citations": 320, "cluster_id": "14317077142483259655", "year": "2001", "url_citations": "http://scholar.google.com/scholar?cites=14317077142483259655&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 11, "url_citation": null, "title": "Adaptive critic designs", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=623201", "url_versions": "http://scholar.google.com/scholar?cluster=2126049297492277631&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "DV Prokhorov, DC Wunsch - Neural Networks, IEEE  \u2026", "excerpt": "(ACD's) for neurocontrol. These are suitable for learning in noisy, nonlinear, and nonstationary environments. They have common roots as generalizations of dynamic programming for neural reinforcement learning approaches. Our discussion of these  ...", "url_pdf": null, "num_citations": 835, "cluster_id": "2126049297492277631", "year": "1997", "url_citations": "http://scholar.google.com/scholar?cites=2126049297492277631&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 18, "url_citation": null, "title": "Technical update: Least-squares temporal difference learning", "url": "http://link.springer.com/article/10.1023/A:1017936530646", "url_versions": "http://scholar.google.com/scholar?cluster=15695765017152043538&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "JA Boyan - Machine Learning", "excerpt": "Abstract TD. \u03bb/is a popular family of algorithms for approximate policy evaluation in large MDPs. TD. \u03bb/works by incrementally updating the value function after each observed transition. It has two major drawbacks: it may make inefficient use of data, and it requires  ...", "url_pdf": null, "num_citations": 249, "cluster_id": "15695765017152043538", "year": "2002", "url_citations": "http://scholar.google.com/scholar?cites=15695765017152043538&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 6, "url_citation": null, "title": "Algorithms for reinforcement learning", "url": "http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009", "url_versions": "http://scholar.google.com/scholar?cluster=2001378641985879784&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "C Szepesv\u00e1ri - \u2026  Lectures on Artificial Intelligence and Machine \u2026", "excerpt": "Abstract Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective. What distinguishes reinforcement learning from supervised learning is that only  ...", "url_pdf": null, "num_citations": 310, "cluster_id": "2001378641985879784", "year": "2010", "url_citations": "http://scholar.google.com/scholar?cites=2001378641985879784&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 27, "url_citation": null, "title": "Predictive reward signal of dopamine neurons", "url": "http://jn.physiology.org/content/80/1/1.short", "url_versions": "http://scholar.google.com/scholar?cluster=12764942088031297390&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "W Schultz - Journal of neurophysiology", "excerpt": "Abstract Schultz, Wolfram. Predictive reward signal of dopamine neurons. J. Neurophysiol. 80: 1\u201327, 1998. The effects of lesions, receptor blocking, electrical self-stimulation, and drugs of abuse suggest that midbrain dopamine systems are involved in processing  ...", "url_pdf": null, "num_citations": 3069, "cluster_id": "12764942088031297390", "year": "1998", "url_citations": "http://scholar.google.com/scholar?cites=12764942088031297390&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 16, "url_citation": null, "title": "Multiple reward signals in the brain", "url": "http://www.nature.com/nrn/journal/v1/n3/abs/nrn1200_199a.html", "url_versions": "http://scholar.google.com/scholar?cluster=530676606458550624&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "W Schultz - Nature reviews neuroscience", "excerpt": "Abstract The fundamental biological importance of rewards has created an increasing interest in the neuronal processing of reward information. The suggestion that the mechanisms underlying drug addiction might involve natural reward systems has also  ...", "url_pdf": null, "num_citations": 1004, "cluster_id": "530676606458550624", "year": "2000", "url_citations": "http://scholar.google.com/scholar?cites=530676606458550624&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 29, "url_citation": null, "title": "Layered learning in multiagent systems: A winning approach to robotic soccer", "url": "http://scholar.google.com/https://books.google.com/books?hl=en&lr=&id=cbjwtjuYIt4C&oi=fnd&pg=PR9&ots=4Rk1xrXQv7&sig=EByv4y5mwRa6HCS2ZI_2yjTtv5A", "url_versions": "http://scholar.google.com/scholar?cluster=9646175575842172758&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "P Stone -", "excerpt": "Multi-agent systems in complex, real-time domains require agents to act effectively both autonomously and as part of a team. This dissertation addresses multi-agent systems consisting of teams of autonomous agents acting in real-time, noisy, collaborative, and  ...", "url_pdf": null, "num_citations": 712, "cluster_id": "9646175575842172758", "year": "1998", "url_citations": "http://scholar.google.com/scholar?cites=9646175575842172758&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 23, "url_citation": null, "title": "Stochastic dynamic programming with factored representations", "url": "http://www.sciencedirect.com/science/article/pii/S0004370200000333", "url_versions": "http://scholar.google.com/scholar?cluster=2024227496110842459&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "C Boutilier, R Dearden, M Goldszmidt - Artificial Intelligence", "excerpt": "Markov decision processes (MDPs) have proven to be popular models for decision-theoretic planning, but standard dynamic programming algorithms for solving MDPs rely on explicit, state-based specifications and computations. To alleviate the combinatorial problems  ...", "url_pdf": null, "num_citations": 413, "cluster_id": "2024227496110842459", "year": "2000", "url_citations": "http://scholar.google.com/scholar?cites=2024227496110842459&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 3, "url_citation": null, "title": "The continuity of mind", "url": "http://scholar.google.com/https://books.google.com/books?hl=en&lr=&id=3griBwAAQBAJ&oi=fnd&pg=PT15&ots=rCVpdXqXcg&sig=p02D6K1ZsOREHDC-m5pO3M30lEw", "url_versions": "http://scholar.google.com/scholar?cluster=2216619366273197074&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "M Spivey -", "excerpt": "The cognitive and neural sciences have been on the brink of a paradigm shift for over a decade. The traditional information-processing framework in psychology, with its computer metaphor of the mind, is still considered to be the mainstream approach, but dynamical- ...", "url_pdf": null, "num_citations": 500, "cluster_id": "2216619366273197074", "year": "2008", "url_citations": "http://scholar.google.com/scholar?cites=2216619366273197074&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 14, "url_citation": null, "title": "Dopamine neurons report an error in the temporal prediction of reward during learning", "url": "http://www.nature.com/neuro/journal/v1/n4/abs/nn0898_304.html", "url_versions": "http://scholar.google.com/scholar?cluster=12306318052739878157&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "JR Hollerman, W Schultz - Nature neuroscience", "excerpt": "Results Dopamine neurons in pars compacta of the substantia nigra and the ventral tegmental area were studied while monkeys learned to associate visual stimuli with liquid reward. Dopamine neurons in these two different midbrain groups showed similar  ...", "url_pdf": null, "num_citations": 821, "cluster_id": "12306318052739878157", "year": "1998", "url_citations": "http://scholar.google.com/scholar?cites=12306318052739878157&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 18, "url_citation": null, "title": "Transfer learning for reinforcement learning domains: A survey", "url": "http://dl.acm.org/citation.cfm?id=1755839", "url_versions": "http://scholar.google.com/scholar?cluster=8985386711450788984&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "ME Taylor, P Stone - The Journal of Machine Learning Research", "excerpt": "Abstract The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to  ...", "url_pdf": null, "num_citations": 330, "cluster_id": "8985386711450788984", "year": "2009", "url_citations": "http://scholar.google.com/scholar?cites=8985386711450788984&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 11, "url_citation": null, "title": "Knightcap: a chess program that learns by combining td (lambda) with game-tree search", "url": "http://arxiv.org/abs/cs/9901002", "url_versions": "http://scholar.google.com/scholar?cluster=13388293501596105761&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "J Baxter, A Tridgell, L Weaver - arXiv preprint cs/9901002", "excerpt": "Abstract: In this paper we present TDLeaf (lambda), a variation on the TD (lambda) algorithm that enables it to be used in conjunction with game-tree search. We present some experiments in which our chess program``KnightCap''used TDLeaf (lambda) to learn its  ...", "url_pdf": null, "num_citations": 112, "cluster_id": "13388293501596105761", "year": "1999", "url_citations": "http://scholar.google.com/scholar?cites=13388293501596105761&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 22, "url_citation": null, "title": "Getting formal with dopamine and reward", "url": "http://www.sciencedirect.com/science/article/pii/S0896627302009674", "url_versions": "http://scholar.google.com/scholar?cluster=16929584406784327130&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "W Schultz - Neuron", "excerpt": "Recent neurophysiological studies reveal that neurons in certain brain structures carry specific signals about past and future rewards. Dopamine neurons display a short-latency, phasic reward signal indicating the difference between actual and predicted rewards. The  ...", "url_pdf": null, "num_citations": 1871, "cluster_id": "16929584406784327130", "year": "2002", "url_citations": "http://scholar.google.com/scholar?cites=16929584406784327130&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 26, "url_citation": null, "title": "On the sample complexity of reinforcement learning", "url": "http://www.ias.tu-darmstadt.de/uploads/Research/NIPS2006/SK.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=6431972069500236915&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "SM Kakade -", "excerpt": "Abstract This thesis is a detailed investigation into the following question: how much data must an agent collect in order to perform \u201creinforcement learning\u201d successfully? This question is analogous to the classical issue of the sample complexity in supervised  ...", "url_pdf": "http://www.ias.tu-darmstadt.de/uploads/Research/NIPS2006/SK.pdf", "num_citations": 204, "cluster_id": "6431972069500236915", "year": "2003", "url_citations": "http://scholar.google.com/scholar?cites=6431972069500236915&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 6, "url_citation": null, "title": "The predictive brain: temporal coincidence and temporal order in synaptic learning mechanisms.", "url": "http://learnmem.cshlp.org/content/1/1/1.short", "url_versions": "http://scholar.google.com/scholar?cluster=17694088014678705745&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "PR Montague, TJ Sejnowski - Learning & Memory", "excerpt": "Abstract Some forms of synaptic plasticity depend on the temporal coincidence of presynaptic activity and postsynaptic response. This requirement is consistent with the Hebbian, or correlational, type of learning rule used in many neural network models.  ...", "url_pdf": null, "num_citations": 172, "cluster_id": "17694088014678705745", "year": "1994", "url_citations": "http://scholar.google.com/scholar?cites=17694088014678705745&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 14, "url_citation": null, "title": "Reinforcement learning with selective perception and hidden state", "url": "http://web.media.mit.edu/%7Etristan/Classes/MAS.945/Papers/Contextual/McCallum_Thesis.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=10987112538182641707&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "AK McCallum -", "excerpt": "Ch a pte r Out line Th isi ntroductory chapter defines the terms\" select i ve percept i on,\"\" hi dden state\" and\" re i nforcement learn i ng.\" It di scusses the need for taskdependent representat i ons, and then outli nes the contr i but i ons of the di ssertat i on.", "url_pdf": "http://web.media.mit.edu/%7Etristan/Classes/MAS.945/Papers/Contextual/McCallum_Thesis.pdf", "num_citations": 516, "cluster_id": "10987112538182641707", "year": "1996", "url_citations": "http://scholar.google.com/scholar?cites=10987112538182641707&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 17, "url_citation": null, "title": "Kernel-based least squares policy iteration for reinforcement learning", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4267723", "url_versions": "http://scholar.google.com/scholar?cluster=16459039293866414400&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "X Xu, D Hu, X Lu - Neural Networks, IEEE Transactions on", "excerpt": "Abstract\u2014In this paper, we present a kernel-based least squares policy iteration (KLSPI) algorithm for reinforcement learning (RL) in large or continuous state spaces, which can be used to realize adaptive feedback control of uncertain dynamic systems. By using KLSPI,  ...", "url_pdf": null, "num_citations": 148, "cluster_id": "16459039293866414400", "year": "2007", "url_citations": "http://scholar.google.com/scholar?cites=16459039293866414400&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 10, "url_citation": null, "title": "Building intelligent interactive tutors: Student-centered strategies for revolutionizing e-learning", "url": "http://scholar.google.com/https://books.google.com/books?hl=en&lr=&id=MnrUj3J_VuEC&oi=fnd&pg=PP2&ots=lwv_k0mw-f&sig=XUTsNtKG5msGIjdZ7hQ1XH-GQVA", "url_versions": "http://scholar.google.com/scholar?cluster=13422163145483859100&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "BP Woolf -", "excerpt": "Computers have transformed every facet of our culture, most dramatically communication, transportation, finance, science, and the economy. Yet their impact has not been generally felt in education due to lack of hardware, teacher training, and sophisticated software.  ...", "url_pdf": null, "num_citations": 408, "cluster_id": "13422163145483859100", "year": "2010", "url_citations": "http://scholar.google.com/scholar?cites=13422163145483859100&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 23, "url_citation": null, "title": "The computational complexity of probabilistic planning", "url": "http://www.jair.org/media/505/live-505-1707-jair.ps.Z", "url_versions": "http://scholar.google.com/scholar?cluster=9298580571683782534&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "ML Littman, J Goldsmith, M Mundhenk - Journal of Artificial Intelligence  \u2026", "excerpt": "Abstract We examine the computational complexity of testing and nding small plans in probabilistic planning domains with both at and propositional representations. The complexity of plan evaluation and existence varies with the plan type sought; we examine  ...", "url_pdf": null, "num_citations": 168, "cluster_id": "9298580571683782534", "year": "1998", "url_citations": "http://scholar.google.com/scholar?cites=9298580571683782534&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 29, "url_citation": null, "title": "Evolutionary function approximation for reinforcement learning", "url": "http://dl.acm.org/citation.cfm?id=1248578", "url_versions": "http://scholar.google.com/scholar?cluster=1844804372793303956&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "S Whiteson, P Stone - The Journal of Machine Learning Research", "excerpt": "Abstract Temporal difference methods are theoretically grounded and empirically effective methods for addressing reinforcement learning problems. In most real-world reinforcement learning tasks, TD methods require a function approximator to represent the value function ...", "url_pdf": null, "num_citations": 210, "cluster_id": "1844804372793303956", "year": "2006", "url_citations": "http://scholar.google.com/scholar?cites=1844804372793303956&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 7, "url_citation": null, "title": "Semisupervised learning for computational linguistics", "url": "http://scholar.google.com/https://books.google.com/books?hl=en&lr=&id=VCd67cGB_rAC&oi=fnd&pg=PP1&ots=Up9wEa-A1a&sig=gXAvNtgSHHLJc-nxXyYcj8EnAEw", "url_versions": "http://scholar.google.com/scholar?cluster=9088080189542925930&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "S Abney -", "excerpt": "The rapid advancement in the theoretical understanding of statistical and machine learning methods for semisupervised learning has made it difficult for nonspecialists to keep up to date in the field. Providing a broad, accessible treatment of the theory as well as linguistic  ...", "url_pdf": null, "num_citations": 153, "cluster_id": "9088080189542925930", "year": "2007", "url_citations": "http://scholar.google.com/scholar?cites=9088080189542925930&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 10, "url_citation": null, "title": "A model of antipsychotic action in conditioned avoidance: A computational approach", "url": "http://graduate.mcmaster.ca/pnb/department/becker/papers/SmithLiBeckerKapur04.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=11858310589510517748&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "A Smith, M Li, S Becker\u2026 -  \u2026", "excerpt": "Conditioned avoidance response (CAR) is one of most important preclinical animal models in the study of antipsychotic drugs (APDs)(Kilts, 2001; Wadenberg and Hicks, 1999). In a typical CAR experiment, a rat is placed in a two-compartment shuttle box and presented  ...", "url_pdf": "http://graduate.mcmaster.ca/pnb/department/becker/papers/SmithLiBeckerKapur04.pdf", "num_citations": 166, "cluster_id": "11858310589510517748", "year": "2004", "url_citations": "http://scholar.google.com/scholar?cites=11858310589510517748&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 6, "url_citation": null, "title": "Improving elevator performance using reinforcement learning", "url": "http://www-public.int-evry.fr/%7Egibson/Teaching/CSC7003/ReadingMaterial/BartoCrites96.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=2288960267030059750&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "A Barto, RH Crites - Advances in neural information  \u2026", "excerpt": "Abstract This paper describes the application of reinforcement learning (RL) to the di\ufb01icult real world problem of elevator dispatching. The el-evator domain poses a combination of challenges not seen in most RL research to date. Elevator systems operate in continuous  ...", "url_pdf": "http://www-public.int-evry.fr/%7Egibson/Teaching/CSC7003/ReadingMaterial/BartoCrites96.pdf", "num_citations": 661, "cluster_id": "2288960267030059750", "year": "1996", "url_citations": "http://scholar.google.com/scholar?cites=2288960267030059750&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 24, "url_citation": null, "title": "Computer Go: an AI oriented survey", "url": "http://www.sciencedirect.com/science/article/pii/S0004370201001278", "url_versions": "http://scholar.google.com/scholar?cluster=11996699811605164768&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "B Bouzy, T Cazenave - Artificial Intelligence", "excerpt": "Since the beginning of AI, mind games have been studied as relevant application fields. Nowadays, some programs are better than human players in most classical games. Their results highlight the efficiency of AI methods that are now quite standard. Such methods  ...", "url_pdf": null, "num_citations": 231, "cluster_id": "11996699811605164768", "year": "2001", "url_citations": "http://scholar.google.com/scholar?cites=11996699811605164768&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 19, "url_citation": null, "title": "The neural basis of human error processing: reinforcement learning, dopamine, and the error-related negativity.", "url": "http://psycnet.apa.org/psycinfo/2002-18225-003", "url_versions": "http://scholar.google.com/scholar?cluster=17963634357287937398&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "CB Holroyd, MGH Coles - Psychological review", "excerpt": "Abstract 1. The authors present a unified account of 2 neural systems concerned with the development and expression of adaptive behaviors: a mesencephalic dopamine system for reinforcement learning and a\" generic\" error-processing system associated with the  ...", "url_pdf": null, "num_citations": 2254, "cluster_id": "17963634357287937398", "year": "2002", "url_citations": "http://scholar.google.com/scholar?cites=17963634357287937398&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 7, "url_citation": null, "title": "Dopamine neurons and their role in reward mechanisms", "url": "http://www.sciencedirect.com/science/article/pii/S0959438897800074", "url_versions": "http://scholar.google.com/scholar?cluster=13048697261743225150&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "W Schultz - Current opinion in neurobiology", "excerpt": "Information related to rewards is processed by a limited number of brain structures. Recent studies have demonstrated that dopamine neurons respond to appetitive events, such as primary rewards and reward-predicting stimuli. Rather than responding unconditionally,  ...", "url_pdf": null, "num_citations": 600, "cluster_id": "13048697261743225150", "year": "1997", "url_citations": "http://scholar.google.com/scholar?cites=13048697261743225150&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 21, "url_citation": null, "title": "Action understanding as inverse planning", "url": "http://www.sciencedirect.com/science/article/pii/S0010027709001607", "url_versions": "http://scholar.google.com/scholar?cluster=11478704181983566675&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "CL Baker, R Saxe, JB Tenenbaum - Cognition", "excerpt": "Humans are adept at inferring the mental states underlying other agents' actions, such as goals, beliefs, desires, emotions and other thoughts. We propose a computational framework based on Bayesian inverse planning for modeling human action  ...", "url_pdf": null, "num_citations": 230, "cluster_id": "11478704181983566675", "year": "2009", "url_citations": "http://scholar.google.com/scholar?cites=11478704181983566675&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 32, "url_citation": null, "title": "Decision-theoretic planning: Structural assumptions and computational leverage", "url": "http://www.jair.org/media/575/live-575-1776-jair.ps.Z", "url_versions": "http://scholar.google.com/scholar?cluster=6601153780624285999&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "C Boutilier, T Dean, S Hanks - Journal of Artificial Intelligence Research", "excerpt": "Abstract Planning under uncertainty is a central problem in the study of automated sequential decision making, and has been addressed by researchers in many di erent elds, including AI planning, decision analysis, operations research, control theory and  ...", "url_pdf": null, "num_citations": 1135, "cluster_id": "6601153780624285999", "year": "1999", "url_citations": "http://scholar.google.com/scholar?cites=6601153780624285999&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 15, "url_citation": null, "title": "Active learning with multiple views", "url": "http://www.jair.org/papers/paper2005.html", "url_versions": "http://scholar.google.com/scholar?cluster=11905819937912084147&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "I Muslea, S Minton, CA Knoblock - Journal of Artificial Intelligence Research", "excerpt": "Abstract Efficient representations and solutions for large decision problems with continuous and discrete variables are among the most important challenges faced by the designers of automated decision support systems. In this paper, we describe a novel hybrid factored  ...", "url_pdf": null, "num_citations": 132, "cluster_id": "11905819937912084147", "year": "2006", "url_citations": "http://scholar.google.com/scholar?cites=11905819937912084147&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 119, "url_citation": null, "title": "Reinforcement learning: A survey", "url": "http://www.jair.org/papers/paper301.html", "url_versions": "http://scholar.google.com/scholar?cluster=4983604491168613713&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "LP Kaelbling, ML Littman, AW Moore - Journal of artificial intelligence  \u2026", "excerpt": "Abstract This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized.  ...", "url_pdf": null, "num_citations": 5018, "cluster_id": "4983604491168613713", "year": "1996", "url_citations": "http://scholar.google.com/scholar?cites=4983604491168613713&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 10, "url_citation": null, "title": "Dynamic modeling and control of supply chain systems: A review", "url": "http://www.sciencedirect.com/science/article/pii/S0305054807000366", "url_versions": "http://scholar.google.com/scholar?cluster=1396079436949397059&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "H Sarimveis, P Patrinos, CD Tarantilis\u2026 - Computers & Operations  \u2026", "excerpt": "Supply chains are complicated dynamical systems triggered by customer demands. Proper selection of equipment, machinery, buildings and transportation fleets is a key component for the success of such systems. However, efficiency of supply chains mostly depends on  ...", "url_pdf": null, "num_citations": 271, "cluster_id": "1396079436949397059", "year": "2008", "url_citations": "http://scholar.google.com/scholar?cites=1396079436949397059&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 15, "url_citation": null, "title": "Learning to trade via direct reinforcement", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=935097", "url_versions": "http://scholar.google.com/scholar?cluster=12827599028049255392&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "J Moody, M Saffell - Neural Networks, IEEE Transactions on", "excerpt": "Abstract\u2014We present methods for optimizing portfolios, asset allocations, and trading systems based on direct reinforcement (DR). In this approach, investment decision making is viewed as a stochastic control problem, and strategies are discovered directly. We present  ...", "url_pdf": null, "num_citations": 140, "cluster_id": "12827599028049255392", "year": "2001", "url_citations": "http://scholar.google.com/scholar?cites=12827599028049255392&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 13, "url_citation": null, "title": "Is a bird in the hand worth two in the future? The neuroeconomics of intertemporal decision-making", "url": "http://www.sciencedirect.com/science/article/pii/S0301008207002158", "url_versions": "http://scholar.google.com/scholar?cluster=5014190291943841482&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "T Kalenscher, CMA Pennartz - Progress in neurobiology", "excerpt": "When making intertemporal decisions, ie, decisions between outcomes occurring at different instants in time, humans and animals prefer rewards with short-term availability over rewards that become available in the long run. Discounted utility theory (DUT) is an  ...", "url_pdf": null, "num_citations": 126, "cluster_id": "5014190291943841482", "year": "2008", "url_citations": "http://scholar.google.com/scholar?cites=5014190291943841482&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 40, "url_citation": null, "title": "Temporal difference learning of position evaluation in the game of Go", "url": "http://www.variational-bayes.org/%7Edayan/papers/sds94.pdf", "url_versions": "http://scholar.google.com/scholar?cluster=6079230982595718678&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "NN Schraudolph, P Dayan\u2026 - Advances in Neural  \u2026", "excerpt": "Abstract The game of Go has a high branching factor that defeats the tree search approach used in computer chess, and long-range spatiotemporal interactions that make position evaluation extremely difficult. Development of conventional Go programs is hampered by  ...", "url_pdf": "http://www.variational-bayes.org/%7Edayan/papers/sds94.pdf", "num_citations": 171, "cluster_id": "6079230982595718678", "year": "1994", "url_citations": "http://scholar.google.com/scholar?cites=6079230982595718678&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 14, "url_citation": null, "title": "A stochastic model of human-machine interaction for learning dialog strategies", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=817450", "url_versions": "http://scholar.google.com/scholar?cluster=15616874569953940305&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "E Levin, R Pieraccini, W Eckert - Speech and Audio Processing \u2026", "excerpt": "Abstract\u2014In this paper, we propose a quantitative model for dialog systems that can be used for learning the dialog strategy. We claim that the problem of dialog design can be formalized as an optimization problem with an objective function reflecting different dialog  ...", "url_pdf": null, "num_citations": 433, "cluster_id": "15616874569953940305", "year": "2000", "url_citations": "http://scholar.google.com/scholar?cites=15616874569953940305&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 9, "url_citation": null, "title": "A neural network model with dopamine-like reinforcement signal that learns a spatial delayed response task", "url": "http://www.sciencedirect.com/science/article/pii/S0306452298006976", "url_versions": "http://scholar.google.com/scholar?cluster=8613640049892698136&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "RE Suri, W Schultz - Neuroscience", "excerpt": "This study investigated how the simulated response of dopamine neurons to reward-related stimuli could be used as reinforcement signal for learning a spatial delayed response task. Spatial delayed response tasks assess the functions of frontal cortex and basal ganglia in  ...", "url_pdf": null, "num_citations": 224, "cluster_id": "8613640049892698136", "year": "1999", "url_citations": "http://scholar.google.com/scholar?cites=8613640049892698136&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 26, "url_citation": null, "title": "Metalearning and neuromodulation", "url": "http://www.sciencedirect.com/science/article/pii/S0893608002000448", "url_versions": "http://scholar.google.com/scholar?cluster=16707744385914625648&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "K Doya - Neural Networks", "excerpt": "This paper presents a computational theory on the roles of the ascending neuromodulatory systems from the viewpoint that they mediate the global signals that regulate the distributed learning mechanisms in the brain. Based on the review of experimental data and  ...", "url_pdf": null, "num_citations": 440, "cluster_id": "16707744385914625648", "year": "2002", "url_citations": "http://scholar.google.com/scholar?cites=16707744385914625648&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 17, "url_citation": null, "title": "Games solved: Now and in the future", "url": "http://www.sciencedirect.com/science/article/pii/S0004370201001527", "url_versions": "http://scholar.google.com/scholar?cluster=15262513140396610187&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "HJ Van den Herik, JWHM Uiterwijk, J Van Rijswijck - Artificial Intelligence", "excerpt": "In this article we present an overview on the state of the art in games solved in the domain of two-person zero-sum games with perfect information. The results are summarized and some predictions for the near future are given. The aim of the article is to determine which game  ...", "url_pdf": null, "num_citations": 203, "cluster_id": "15262513140396610187", "year": "2002", "url_citations": "http://scholar.google.com/scholar?cites=15262513140396610187&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 21, "url_citation": null, "title": "Reinforcement learning in continuous time and space", "url": "http://www.mitpressjournals.org/doi/abs/10.1162/089976600300015961", "url_versions": "http://scholar.google.com/scholar?cluster=6940306061526307175&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "K Doya - Neural computation", "excerpt": "The performance of the proposed algorithms is first tested in a nonlinear control task of swinging a pendulum up with limited torque. It is shown in the simulations that (1) the task is accomplished by the continuous actor-critic method in a number of trials several times  ...", "url_pdf": null, "num_citations": 569, "cluster_id": "6940306061526307175", "year": "2000", "url_citations": "http://scholar.google.com/scholar?cites=6940306061526307175&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 22, "url_citation": null, "title": "Infinite-horizon policy-gradient estimation", "url": "http://www.jair.org/papers/paper806.html", "url_versions": "http://scholar.google.com/scholar?cluster=9191776830518090033&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "J Baxter, PL Bartlett - Journal of Artificial Intelligence Research", "excerpt": "Abstract Gradient-based approaches to direct policy search in reinforcement learning have received much recent attention as a means to solve problems of partial observability and to avoid some of the problems associated with policy degradation in value-function methods.  ...", "url_pdf": null, "num_citations": 470, "cluster_id": "9191776830518090033", "year": "2001", "url_citations": "http://scholar.google.com/scholar?cites=9191776830518090033&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 17, "url_citation": null, "title": "Direct gradient-based reinforcement learning", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=856049", "url_versions": "http://scholar.google.com/scholar?cluster=14984884921549577932&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "J Baxter, PL Bartlett - \u2026 . ISCAS", "excerpt": "Abstract Many control, scheduling, planning and game-playing tasks can be formulated as reinforcement learning problems, in which an agent chooses actions to take in some environment, aiming to maximize a reward function. We present an algorithm for  ...", "url_pdf": null, "num_citations": 166, "cluster_id": "14984884921549577932", "year": "2000", "url_citations": "http://scholar.google.com/scholar?cites=14984884921549577932&as_sdt=2005&sciodt=1,5&hl=en"}]