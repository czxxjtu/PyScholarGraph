[{"num_versions": 0, "url_citation": null, "title": "The SP theory of intelligence: distinctive features and advantages", "url": "http://www.cognitionresearch.org/papers/overview/spalt.pdf", "url_versions": null, "authors": "JG Wolff -", "excerpt": "Abstract The main aim of this paper is to highlight distinctive features of the SP theory of intelligence, outlined in an appendix with pointers to fuller information elsewhere, and its apparent advantages compared with some AI-related alternatives. In summary, distinctive  ...", "url_pdf": "http://www.cognitionresearch.org/papers/overview/spalt.pdf", "num_citations": 0, "cluster_id": null, "year": "2015", "url_citations": null}, {"num_versions": 10, "url_citation": null, "title": "Do deep nets really need to be deep?", "url": "http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep", "url_versions": "http://scholar.google.com/scholar?cluster=207936000892324125&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "J Ba, R Caruana - Advances in Neural Information Processing  \u2026", "excerpt": "Abstract Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and  ...", "url_pdf": null, "num_citations": 33, "cluster_id": "207936000892324125", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=207936000892324125&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 2, "url_citation": null, "title": "Neuron clustering for mitigating catastrophic forgetting in feedforward neural networks", "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7007868", "url_versions": "http://scholar.google.com/scholar?cluster=6813118852957063605&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "B Goodrich, I Arel - Computational Intelligence in Dynamic and  \u2026", "excerpt": "Abstract-Catastrophic forgetting is a fundamental problem with artificial neural networks (ANNs) in which learned rep resentations are lost as new representations are acquired. This significantly limits the usefulness of ANNs in dynamic or non stationary settings, as well as  ...", "url_pdf": null, "num_citations": 1, "cluster_id": "6813118852957063605", "year": "2014", "url_citations": "http://scholar.google.com/scholar?cites=6813118852957063605&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 3, "url_citation": null, "title": "Knowledge matters: Importance of prior information for optimization", "url": "http://arxiv.org/abs/1301.4083", "url_versions": "http://scholar.google.com/scholar?cluster=16555444433184520296&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "\u00c7 G\u00fcl\u00e7ehre, Y Bengio - arXiv preprint arXiv:1301.4083", "excerpt": "Abstract: We explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state-of-the-art machine learning algorithms tested failed to learn. We motivate our work from the hypothesis that humans  ...", "url_pdf": null, "num_citations": 19, "cluster_id": "16555444433184520296", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=16555444433184520296&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 7, "url_citation": null, "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "url": "http://arxiv.org/abs/1312.6120", "url_versions": "http://scholar.google.com/scholar?cluster=9090095758382098911&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "AM Saxe, JL McClelland, S Ganguli - arXiv preprint arXiv:1312.6120", "excerpt": "Abstract: Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by  ...", "url_pdf": null, "num_citations": 31, "cluster_id": "9090095758382098911", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=9090095758382098911&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 10, "url_citation": null, "title": "Deep learning of representations: Looking forward", "url": "http://link.springer.com/chapter/10.1007/978-3-642-39593-2_1", "url_versions": "http://scholar.google.com/scholar?cluster=16988628068303769209&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "Y Bengio - Statistical Language and Speech Processing", "excerpt": "Abstract Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts. Although the study of deep learning has already led to impressive theoretical  ...", "url_pdf": null, "num_citations": 86, "cluster_id": "16988628068303769209", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=16988628068303769209&as_sdt=2005&sciodt=1,5&hl=en"}, {"num_versions": 6, "url_citation": null, "title": "Training neural networks with stochastic hessian-free optimization", "url": "http://arxiv.org/abs/1301.3641", "url_versions": "http://scholar.google.com/scholar?cluster=15998932101819720935&hl=en&as_sdt=1,5&sciodt=1,5&as_vis=1", "authors": "R Kiros - arXiv preprint arXiv:1301.3641", "excerpt": "Abstract: Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same  ...", "url_pdf": null, "num_citations": 5, "cluster_id": "15998932101819720935", "year": "2013", "url_citations": "http://scholar.google.com/scholar?cites=15998932101819720935&as_sdt=2005&sciodt=1,5&hl=en"}]